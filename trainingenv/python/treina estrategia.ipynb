{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bc71e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "from py4j.java_gateway import JavaGateway\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "534058d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conecta no servidor de treinamento\n",
    "\n",
    "gateway = JavaGateway()\n",
    "minitruco_java = gateway.entry_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c88c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chesterbr/.pyenv/versions/3.7.17/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# Env customizado do gym que encapsupla o servidor\n",
    "\n",
    "class MinitrucoEnv(Env):\n",
    "    def __init__(self):\n",
    "        # Ações que podemos tomar: jogar uma carta de valor 0 até uma carta de valor 14\n",
    "        self.action_space = Discrete(15)\n",
    "\n",
    "        # Espaço de observação: os 3 primeiros elementos são as cartas em mãos,\n",
    "        # o restante são os valores das cartas na mesa, pontuações, etc.\n",
    "        # Detalhes em SituacaoJogo.java#ranges e #toObservacao\n",
    "        low, high = np.array(minitruco_java.ranges()).transpose()\n",
    "        self.observation_space = Box(low=np.array(low), high=np.array(high), dtype=np.float32)\n",
    "\n",
    "        self.episodio = None\n",
    "        self.state = None\n",
    "\n",
    "    def _get_obs(self):\n",
    "        values = self.episodio.estado().split()\n",
    "        return [float(val) for val in values]\n",
    "        # return np.array(float_values[24:27], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        action = int(action)\n",
    "        last_observation = self.state\n",
    "        indice_carta = -1\n",
    "        for i in range(3):\n",
    "            if last_observation[i] == action:\n",
    "                indice_carta = i\n",
    "                break\n",
    "        if indice_carta == -1:\n",
    "            # Jogada inválida (não deve acontecer no Keras por causa do masking, mas\n",
    "            # se rolar por conta do teste inicial, só ignora e mantém no mesmo estado)\n",
    "            return last_observation, 0, False, {}\n",
    "\n",
    "        self.episodio.executa(indice_carta)\n",
    "        estado_str = self.episodio.estado()\n",
    "        if estado_str == \"EQUIPE 1 VENCEU\":\n",
    "            return last_observation, 1.0, True, {}\n",
    "        if estado_str == \"EQUIPE 2 VENCEU\":\n",
    "            return last_observation, -1.0, True, {}\n",
    "\n",
    "        self.state = self._get_obs()\n",
    "        ganhoPontosEquipe1 = self.state[13] - last_observation[13]\n",
    "        ganhoPontosEquipe2 = self.state[14] - last_observation[14]\n",
    "\n",
    "        # Dá 1 ponto de recompensa se a equipe 1 ganhou pontos e -1 se a equipe 2 ganhou pontos\n",
    "        reward = ganhoPontosEquipe1 - ganhoPontosEquipe2\n",
    "        terminated = False\n",
    "\n",
    "        return self.state, reward, terminated, {}\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if self.episodio is not None:\n",
    "            self.episodio.finaliza()\n",
    "\n",
    "#         super().reset(seed=seed) # required by check_env\n",
    "\n",
    "        self.episodio = minitruco_java.novoEpisodio()\n",
    "        self.state = self._get_obs()\n",
    "\n",
    "        return self.state\n",
    "\n",
    "\n",
    "env = MinitrucoEnv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19da8770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:1.0 Time:0.44654011726379395s\n",
      "Episode:2 Score:-1.0 Time:0.46459007263183594s\n",
      "Episode:3 Score:-1.0 Time:0.45887279510498047s\n",
      "Episode:4 Score:1.0 Time:0.44832515716552734s\n",
      "Episode:5 Score:-1.0 Time:0.5011961460113525s\n",
      "Episode:6 Score:1.0 Time:0.48083996772766113s\n",
      "Episode:7 Score:-1.0 Time:0.45574307441711426s\n",
      "Episode:8 Score:-1.0 Time:0.40296292304992676s\n",
      "Episode:9 Score:-1.0 Time:0.49876880645751953s\n",
      "Episode:10 Score:-9.0 Time:0.47785305976867676s\n",
      "Episode:11 Score:1.0 Time:0.44848012924194336s\n",
      "Episode:12 Score:-1.0 Time:0.4601871967315674s\n",
      "Episode:13 Score:1.0 Time:0.386016845703125s\n",
      "Episode:14 Score:-1.0 Time:0.45891785621643066s\n",
      "Episode:15 Score:-1.0 Time:0.46082401275634766s\n",
      "Episode:16 Score:1.0 Time:0.45673179626464844s\n",
      "Episode:17 Score:-1.0 Time:0.49923205375671387s\n",
      "Episode:18 Score:-1.0 Time:0.43932223320007324s\n",
      "Episode:19 Score:-1.0 Time:0.4599759578704834s\n",
      "Episode:20 Score:1.0 Time:0.48914003372192383s\n",
      "Episode:21 Score:-16.0 Time:0.37054896354675293s\n",
      "Episode:22 Score:1.0 Time:0.47051000595092773s\n",
      "Episode:23 Score:-1.0 Time:0.5055758953094482s\n",
      "Episode:24 Score:1.0 Time:0.45694804191589355s\n",
      "Episode:25 Score:-1.0 Time:0.48435401916503906s\n",
      "Episode:26 Score:-1.0 Time:0.49672508239746094s\n",
      "Episode:27 Score:-1.0 Time:0.3761098384857178s\n",
      "Episode:28 Score:1.0 Time:0.5004360675811768s\n",
      "Episode:29 Score:-16.0 Time:0.41657090187072754s\n",
      "Episode:30 Score:-1.0 Time:0.5561296939849854s\n",
      "Episode:31 Score:-1.0 Time:0.409160852432251s\n",
      "Episode:32 Score:-1.0 Time:0.44295287132263184s\n",
      "Episode:33 Score:1.0 Time:0.42851996421813965s\n",
      "Episode:34 Score:-7.0 Time:0.4090099334716797s\n",
      "Episode:35 Score:-12.0 Time:0.43621206283569336s\n",
      "Episode:36 Score:-9.0 Time:0.35503697395324707s\n",
      "Episode:37 Score:-1.0 Time:0.4182150363922119s\n",
      "Episode:38 Score:1.0 Time:0.4210700988769531s\n",
      "Episode:39 Score:-1.0 Time:0.45041680335998535s\n",
      "Episode:40 Score:1.0 Time:0.41958093643188477s\n",
      "Episode:41 Score:-1.0 Time:0.3919668197631836s\n",
      "Episode:42 Score:1.0 Time:0.46556520462036133s\n",
      "Episode:43 Score:1.0 Time:0.4626948833465576s\n",
      "Episode:44 Score:1.0 Time:0.4816408157348633s\n",
      "Episode:45 Score:-1.0 Time:0.48967409133911133s\n",
      "Episode:46 Score:1.0 Time:0.541513204574585s\n",
      "Episode:47 Score:-1.0 Time:0.434215784072876s\n",
      "Episode:48 Score:-1.0 Time:0.37237095832824707s\n",
      "Episode:49 Score:-8.0 Time:0.5363519191741943s\n",
      "Episode:50 Score:-1.0 Time:0.4540858268737793s\n",
      "Average reward per episode -1.72\n"
     ]
    }
   ],
   "source": [
    "# Roda alguns episódios com política \"jogue uma carta aleatoriamente\"\n",
    "\n",
    "import time\n",
    "\n",
    "episodes = 50\n",
    "total_reward = 0\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    terminated = False\n",
    "    score = 0\n",
    "    start = time.time()\n",
    "\n",
    "    while not terminated:\n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, terminated, info = env.step(action)\n",
    "        score+=reward\n",
    "    total_reward += score\n",
    "    print('Episode:{} Score:{} Time:{}s'.format(episode, score, time.time() - start))\n",
    "print(\"Average reward per episode {}\".format(total_reward/episodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca969840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                432       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 15)                375       \n",
      "=================================================================\n",
      "Total params: 1,407\n",
      "Trainable params: 1,407\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "\n",
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1, states[0])))\n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model\n",
    "\n",
    "model = build_model(states, actions)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "143f39f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 15:55:30.148507: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-23 15:55:30.162679: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f910d2185a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-23 15:55:30.162695: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "WARNING:tensorflow:From /Users/chesterbr/.pyenv/versions/3.7.17/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "10000/10000 [==============================] - 154s 15ms/step - reward: -0.0228\n",
      "198 episodes - episode_reward: -1.152 [-16.000, 1.000] - loss: 183038.838 - mae: 1504.938 - mean_q: 2015.875\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 165s 16ms/step - reward: -0.0198\n",
      "197 episodes - episode_reward: -1.005 [-16.000, 1.000] - loss: 523983159296.000 - mae: 1957453.375 - mean_q: 2668181.250\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 163s 16ms/step - reward: -0.0198\n",
      "197 episodes - episode_reward: -1.005 [-15.000, 1.000] - loss: 107973061902336.000 - mae: 36252508.000 - mean_q: 49008188.000\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 163s 16ms/step - reward: -0.0234\n",
      "198 episodes - episode_reward: -1.162 [-15.000, 1.000] - loss: 1534724153540608.000 - mae: 148422464.000 - mean_q: 198731248.000\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 167s 17ms/step - reward: -0.0248\n",
      "200 episodes - episode_reward: -1.260 [-16.000, 1.000] - loss: 9274128250961920.000 - mae: 372014208.000 - mean_q: 495839776.000\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 171s 17ms/step - reward: -0.0227\n",
      "202 episodes - episode_reward: -1.124 [-15.000, 1.000] - loss: 34064016984768512.000 - mae: 725829888.000 - mean_q: 959610944.000\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 175s 17ms/step - reward: -0.0276\n",
      "197 episodes - episode_reward: -1.401 [-16.000, 1.000] - loss: 97723244856999936.000 - mae: 1233475968.000 - mean_q: 1627663488.000\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 169s 17ms/step - reward: -0.0246\n",
      "197 episodes - episode_reward: -1.249 [-15.000, 1.000] - loss: 235657588446855168.000 - mae: 1928116992.000 - mean_q: 2539229184.000\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 168s 17ms/step - reward: -0.0226\n",
      "199 episodes - episode_reward: -1.136 [-16.000, 1.000] - loss: 517135915232002048.000 - mae: 2840687872.000 - mean_q: 3749054208.000\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 169s 17ms/step - reward: -0.0191\n",
      "198 episodes - episode_reward: -0.965 [-16.000, 1.000] - loss: 1031500511406194688.000 - mae: 4019325952.000 - mean_q: 5305392128.000\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 168s 17ms/step - reward: -0.0239\n",
      "200 episodes - episode_reward: -1.195 [-15.000, 1.000] - loss: 1955038326813622272.000 - mae: 5475920384.000 - mean_q: 7249054208.000\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 178s 18ms/step - reward: -0.0233\n",
      "197 episodes - episode_reward: -1.183 [-16.000, 1.000] - loss: 3453678447908356096.000 - mae: 7273009664.000 - mean_q: 9629432832.000\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 191s 19ms/step - reward: -0.0202\n",
      "198 episodes - episode_reward: -1.020 [-16.000, 1.000] - loss: 5726437976875991040.000 - mae: 9397391360.000 - mean_q: 12422515712.000\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 193s 19ms/step - reward: -0.0237\n",
      "199 episodes - episode_reward: -1.191 [-16.000, 1.000] - loss: 9018758420483801088.000 - mae: 11861599232.000 - mean_q: 15626129408.000\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 192s 19ms/step - reward: -0.0173\n",
      "197 episodes - episode_reward: -0.878 [-13.000, 1.000] - loss: 13499244114415190016.000 - mae: 14589636608.000 - mean_q: 19175833600.000\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 193s 19ms/step - reward: -0.0186\n",
      "199 episodes - episode_reward: -0.935 [-14.000, 1.000] - loss: 19899625544513224704.000 - mae: 17725595648.000 - mean_q: 23261784064.000\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: -0.0244\n",
      "199 episodes - episode_reward: -1.226 [-16.000, 1.000] - loss: 28272531934353031168.000 - mae: 21262370816.000 - mean_q: 27942017024.000\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 467s 47ms/step - reward: -0.0188\n",
      "195 episodes - episode_reward: -0.964 [-13.000, 1.000] - loss: 39600617716398948352.000 - mae: 25319266304.000 - mean_q: 33228783616.000\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 157s 16ms/step - reward: -0.0235\n",
      "198 episodes - episode_reward: -1.187 [-16.000, 1.000] - loss: 55127819768781864960.000 - mae: 29744074752.000 - mean_q: 38979567616.000\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 159s 16ms/step - reward: -0.0209\n",
      "201 episodes - episode_reward: -1.040 [-14.000, 1.000] - loss: 73636294823321272320.000 - mae: 34600833024.000 - mean_q: 45318021120.000\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 169s 17ms/step - reward: -0.0237\n",
      "201 episodes - episode_reward: -1.179 [-16.000, 1.000] - loss: 96363046237823303680.000 - mae: 39847944192.000 - mean_q: 52044492800.000\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 165s 16ms/step - reward: -0.0350\n",
      "196 episodes - episode_reward: -1.786 [-16.000, 1.000] - loss: 126795399440151609344.000 - mae: 45558587392.000 - mean_q: 59424165888.000\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 184s 18ms/step - reward: -0.0223\n",
      "197 episodes - episode_reward: -1.132 [-16.000, 1.000] - loss: 162694084651131076608.000 - mae: 51757801472.000 - mean_q: 67511451648.000\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 186s 19ms/step - reward: -0.0214\n",
      "200 episodes - episode_reward: -1.070 [-16.000, 1.000] - loss: 211533001374243487744.000 - mae: 58410717184.000 - mean_q: 76308389888.000\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 187s 19ms/step - reward: -0.0232\n",
      "198 episodes - episode_reward: -1.172 [-16.000, 1.000] - loss: 266854955314071994368.000 - mae: 65967898624.000 - mean_q: 86273105920.000\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 189s 19ms/step - reward: -0.0217\n",
      "199 episodes - episode_reward: -1.085 [-16.000, 1.000] - loss: 341834294794628628480.000 - mae: 74236321792.000 - mean_q: 97174659072.000\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 185s 19ms/step - reward: -0.0207\n",
      "198 episodes - episode_reward: -1.051 [-16.000, 1.000] - loss: 429745615052063375360.000 - mae: 83061727232.000 - mean_q: 108879683584.000\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 392s 39ms/step - reward: -0.0260\n",
      "197 episodes - episode_reward: -1.320 [-16.000, 1.000] - loss: 536623739587052699648.000 - mae: 92666150912.000 - mean_q: 121502179328.000\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 391s 39ms/step - reward: -0.0239\n",
      "196 episodes - episode_reward: -1.219 [-16.000, 1.000] - loss: 660958977362008997888.000 - mae: 103224549376.000 - mean_q: 135427874816.000\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 462s 46ms/step - reward: -0.0209\n",
      "198 episodes - episode_reward: -1.056 [-16.000, 1.000] - loss: 822282067770326646784.000 - mae: 114295562240.000 - mean_q: 150218293248.000\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 315s 31ms/step - reward: -0.0199\n",
      "200 episodes - episode_reward: -0.995 [-16.000, 1.000] - loss: 1011159447336292188160.000 - mae: 126660894720.000 - mean_q: 166683852800.000\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 387s 39ms/step - reward: -0.0163\n",
      "200 episodes - episode_reward: -0.815 [-16.000, 1.000] - loss: 1235200580949045673984.000 - mae: 139848728576.000 - mean_q: 184381571072.000\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 484s 48ms/step - reward: -0.0226\n",
      "202 episodes - episode_reward: -1.119 [-15.000, 1.000] - loss: 1505685085719057399808.000 - mae: 153826705408.000 - mean_q: 202820698112.000\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 492s 49ms/step - reward: -0.0244\n",
      "197 episodes - episode_reward: -1.239 [-15.000, 1.000] - loss: 1818434887829522743296.000 - mae: 168305868800.000 - mean_q: 222122983424.000\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 490s 49ms/step - reward: -0.0210\n",
      "200 episodes - episode_reward: -1.050 [-15.000, 1.000] - loss: 2169087406616403247104.000 - mae: 183668621312.000 - mean_q: 242282037248.000\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 490s 49ms/step - reward: -0.0208\n",
      "198 episodes - episode_reward: -1.051 [-16.000, 1.000] - loss: 2535097903820058918912.000 - mae: 199778467840.000 - mean_q: 263397539840.000\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 490s 49ms/step - reward: -0.0208\n",
      "197 episodes - episode_reward: -1.056 [-15.000, 1.000] - loss: 2987693340822144548864.000 - mae: 216999985152.000 - mean_q: 286248108032.000\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 492s 49ms/step - reward: -0.0232\n",
      "197 episodes - episode_reward: -1.178 [-16.000, 1.000] - loss: 3525930635713191084032.000 - mae: 235248222208.000 - mean_q: 310160064512.000\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 490s 49ms/step - reward: -0.0273\n",
      "195 episodes - episode_reward: -1.400 [-16.000, 1.000] - loss: 4135496785927461666816.000 - mae: 255060262912.000 - mean_q: 336220127232.000\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 491s 49ms/step - reward: -0.0210\n",
      "199 episodes - episode_reward: -1.055 [-16.000, 1.000] - loss: 4827286280438542237696.000 - mae: 275908788224.000 - mean_q: 363575050240.000\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 490s 49ms/step - reward: -0.0221\n",
      "199 episodes - episode_reward: -1.111 [-16.000, 1.000] - loss: 5605663107285354807296.000 - mae: 297790767104.000 - mean_q: 392095793152.000\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 394s 39ms/step - reward: -0.0353\n",
      "200 episodes - episode_reward: -1.765 [-16.000, 1.000] - loss: 6481361596179736952832.000 - mae: 320325517312.000 - mean_q: 421837275136.000\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 492s 49ms/step - reward: -0.0258\n",
      "200 episodes - episode_reward: -1.290 [-16.000, 1.000] - loss: 7510814102252454674432.000 - mae: 343438098432.000 - mean_q: 451933569024.000\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 490s 49ms/step - reward: -0.0274\n",
      "198 episodes - episode_reward: -1.384 [-16.000, 1.000] - loss: 8508973909264341925888.000 - mae: 368062595072.000 - mean_q: 484025106432.000\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 492s 49ms/step - reward: -0.0211\n",
      "198 episodes - episode_reward: -1.066 [-15.000, 1.000] - loss: 9849738302528998604800.000 - mae: 394774052864.000 - mean_q: 519296221184.000\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 492s 49ms/step - reward: -0.0275\n",
      "198 episodes - episode_reward: -1.389 [-13.000, 1.000] - loss: 11257143585379763355648.000 - mae: 422028869632.000 - mean_q: 554825613312.000\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 487s 49ms/step - reward: -0.0215\n",
      "197 episodes - episode_reward: -1.091 [-16.000, 1.000] - loss: 12856034043086973698048.000 - mae: 450895052800.000 - mean_q: 592601481216.000\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 492s 49ms/step - reward: -0.0330\n",
      "197 episodes - episode_reward: -1.675 [-16.000, 1.000] - loss: 14552600821312409042944.000 - mae: 479718277120.000 - mean_q: 630153412608.000\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 492s 49ms/step - reward: -0.0206\n",
      "200 episodes - episode_reward: -1.030 [-12.000, 1.000] - loss: 16502184330203146223616.000 - mae: 510353768448.000 - mean_q: 670756962304.000\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 490s 49ms/step - reward: -0.0301\n",
      "201 episodes - episode_reward: -1.498 [-16.000, 1.000] - loss: 18764808545592779210752.000 - mae: 541234397184.000 - mean_q: 711997521920.000\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 490s 49ms/step - reward: -0.0203\n",
      "198 episodes - episode_reward: -1.025 [-16.000, 1.000] - loss: 21085833189150339104768.000 - mae: 574653333504.000 - mean_q: 756505706496.000\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      " 7471/10000 [=====================>........] - ETA: 2:04 - reward: -0.0158"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities contain NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pf/3rrm384n0px745nn4lbs8knr0000gn/T/ipykernel_3243/2770002046.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Original era 50K steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1_000_000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.7.17/lib/python3.7/site-packages/rl/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;31m# This is were all of the work happens. We first perceive and compute the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;31m# (forward step) and then use the reward to improve (backward step).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.17/lib/python3.7/site-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.17/lib/python3.7/site-packages/rl/policy.py\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, q_values)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mexp_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_values\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities contain NaN"
     ]
    }
   ],
   "source": [
    "class MaskingDQNAgent(DQNAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MaskingDQNAgent, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def compute_q_values(self, state):\n",
    "        q_values = super().compute_q_values(state)\n",
    "        mask = np.array([1 if self._is_action_valid(state, action) else -np.inf for action in range(self.nb_actions)])\n",
    "        masked_q_values = q_values + mask\n",
    "        return masked_q_values\n",
    "\n",
    "    def _is_action_valid(self, state, action):\n",
    "        return state[0][0] == action or state[0][1] == action or state[0][2] == action\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = MaskingDQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  nb_actions=actions, nb_steps_warmup=1000, target_model_update=1e-2)\n",
    "    return dqn\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "# Original era 50K steps\n",
    "dqn.fit(env, nb_steps=1_000_000, visualize=False, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2663a67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 50 episodes ...\n",
      "Episode 1: reward: -10.000, steps: 45\n",
      "Episode 2: reward: 1.000, steps: 56\n",
      "Episode 3: reward: 1.000, steps: 51\n",
      "Episode 4: reward: -1.000, steps: 40\n",
      "Episode 5: reward: -1.000, steps: 56\n",
      "Episode 6: reward: 1.000, steps: 54\n",
      "Episode 7: reward: 1.000, steps: 59\n",
      "Episode 8: reward: -1.000, steps: 51\n",
      "Episode 9: reward: -1.000, steps: 53\n",
      "Episode 10: reward: -1.000, steps: 54\n",
      "Episode 11: reward: -1.000, steps: 56\n",
      "Episode 12: reward: -1.000, steps: 51\n",
      "Episode 13: reward: -1.000, steps: 46\n",
      "Episode 14: reward: -1.000, steps: 55\n",
      "Episode 15: reward: -1.000, steps: 43\n",
      "Episode 16: reward: -1.000, steps: 46\n",
      "Episode 17: reward: -1.000, steps: 57\n",
      "Episode 18: reward: -7.000, steps: 54\n",
      "Episode 19: reward: 1.000, steps: 60\n",
      "Episode 20: reward: -1.000, steps: 44\n",
      "Episode 21: reward: -1.000, steps: 38\n",
      "Episode 22: reward: 1.000, steps: 52\n",
      "Episode 23: reward: -1.000, steps: 53\n",
      "Episode 24: reward: -1.000, steps: 56\n",
      "Episode 25: reward: 1.000, steps: 49\n",
      "Episode 26: reward: -1.000, steps: 48\n",
      "Episode 27: reward: 1.000, steps: 54\n",
      "Episode 28: reward: -13.000, steps: 56\n",
      "Episode 29: reward: -1.000, steps: 40\n",
      "Episode 30: reward: -1.000, steps: 50\n",
      "Episode 31: reward: -1.000, steps: 49\n",
      "Episode 32: reward: 1.000, steps: 56\n",
      "Episode 33: reward: 1.000, steps: 56\n",
      "Episode 34: reward: 1.000, steps: 53\n",
      "Episode 35: reward: 1.000, steps: 59\n",
      "Episode 36: reward: -1.000, steps: 41\n",
      "Episode 37: reward: 1.000, steps: 54\n",
      "Episode 38: reward: -1.000, steps: 50\n",
      "Episode 39: reward: 0.000, steps: 53\n",
      "Episode 40: reward: -1.000, steps: 56\n",
      "Episode 41: reward: -1.000, steps: 53\n",
      "Episode 42: reward: -1.000, steps: 53\n",
      "Episode 43: reward: -1.000, steps: 56\n",
      "Episode 44: reward: -1.000, steps: 46\n",
      "Episode 45: reward: 1.000, steps: 49\n",
      "Episode 46: reward: 1.000, steps: 49\n",
      "Episode 47: reward: -1.000, steps: 59\n",
      "Episode 48: reward: -1.000, steps: 55\n",
      "Episode 49: reward: -1.000, steps: 49\n",
      "Episode 50: reward: 1.000, steps: 49\n",
      "Average reward per episode: -0.88\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=50, visualize=False)\n",
    "print(\"Average reward per episode: {}\".format(np.mean(scores.history['episode_reward'])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
