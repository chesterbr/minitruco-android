{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bc71e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "from py4j.java_gateway import JavaGateway\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "534058d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conecta no servidor de treinamento\n",
    "\n",
    "gateway = JavaGateway()\n",
    "minitruco_java = gateway.entry_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c88c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env customizado do gym que encapsupla o servidor\n",
    "\n",
    "class MinitrucoEnv(Env):\n",
    "    def __init__(self):\n",
    "        # Ações que podemos tomar: jogar uma carta de valor 0 até uma carta de valor 14\n",
    "        self.action_space = Discrete(15)\n",
    "\n",
    "        # Espaço de observação: os 3 primeiros elementos são as cartas em mãos,\n",
    "        # o restante são os valores das cartas na mesa, pontuações, etc.\n",
    "        # Detalhes em SituacaoJogo.java#ranges e #toObservacao\n",
    "        low, high = np.array(minitruco_java.ranges()).transpose()\n",
    "        self.observation_space = Box(low=np.float32(np.array(low)), high=np.float32(np.array(high)))\n",
    "\n",
    "        self.episodio = None\n",
    "        self.state = None\n",
    "\n",
    "    def _get_obs(self):\n",
    "        values = self.episodio.estado().split()\n",
    "        return [float(val) for val in values]\n",
    "        # return np.array(float_values[24:27], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        action = int(action)\n",
    "        last_observation = self.state\n",
    "        indice_carta = -1\n",
    "        for i in range(3):\n",
    "            if last_observation[i] == action:\n",
    "                indice_carta = i\n",
    "                break\n",
    "        if indice_carta == -1:\n",
    "            # Jogada inválida (não deve acontecer no Keras por causa do masking, mas\n",
    "            # se rolar por conta do teste inicial, só ignora e mantém no mesmo estado)\n",
    "            return last_observation, 0, False, {}\n",
    "\n",
    "        self.episodio.executa(indice_carta)\n",
    "        estado_str = self.episodio.estado()\n",
    "        if estado_str == \"EQUIPE 1 VENCEU\":\n",
    "            return last_observation, 1.0, True, {}\n",
    "        if estado_str == \"EQUIPE 2 VENCEU\":\n",
    "            return last_observation, -1.0, True, {}\n",
    "\n",
    "        self.state = self._get_obs()\n",
    "        ganhoPontosEquipe1 = self.state[13] - last_observation[13]\n",
    "        ganhoPontosEquipe2 = self.state[14] - last_observation[14]\n",
    "\n",
    "        # Dá 1 ponto de recompensa se a equipe 1 ganhou pontos e -1 se a equipe 2 ganhou pontos\n",
    "        reward = ganhoPontosEquipe1 - ganhoPontosEquipe2\n",
    "        terminated = False\n",
    "\n",
    "        return self.state, reward, terminated, {}\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if self.episodio is not None:\n",
    "            self.episodio.finaliza()\n",
    "\n",
    "#         super().reset(seed=seed) # required by check_env\n",
    "\n",
    "        self.episodio = minitruco_java.novoEpisodio()\n",
    "        self.state = self._get_obs()\n",
    "\n",
    "        return self.state\n",
    "\n",
    "\n",
    "env = MinitrucoEnv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19da8770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-1.0 Time:0.5723845958709717s\n",
      "Episode:2 Score:-1.0 Time:0.46492505073547363s\n",
      "Episode:3 Score:-13.0 Time:0.37071990966796875s\n",
      "Episode:4 Score:-1.0 Time:0.3986818790435791s\n",
      "Episode:5 Score:1.0 Time:0.5000889301300049s\n",
      "Episode:6 Score:-1.0 Time:0.40629100799560547s\n",
      "Episode:7 Score:1.0 Time:0.3818390369415283s\n",
      "Episode:8 Score:-1.0 Time:0.3865320682525635s\n",
      "Episode:9 Score:1.0 Time:0.4010961055755615s\n",
      "Episode:10 Score:1.0 Time:0.43812108039855957s\n",
      "Episode:11 Score:-1.0 Time:0.42306089401245117s\n",
      "Episode:12 Score:1.0 Time:0.37663888931274414s\n",
      "Episode:13 Score:-1.0 Time:0.421950101852417s\n",
      "Episode:14 Score:-1.0 Time:0.2702200412750244s\n",
      "Episode:15 Score:-1.0 Time:0.3798191547393799s\n",
      "Episode:16 Score:1.0 Time:0.42183685302734375s\n",
      "Episode:17 Score:-1.0 Time:0.4342000484466553s\n",
      "Episode:18 Score:-15.0 Time:0.42229723930358887s\n",
      "Episode:19 Score:-1.0 Time:0.39779019355773926s\n",
      "Episode:20 Score:-1.0 Time:0.43346214294433594s\n",
      "Episode:21 Score:-1.0 Time:0.4246330261230469s\n",
      "Episode:22 Score:-1.0 Time:0.38416409492492676s\n",
      "Episode:23 Score:1.0 Time:0.424468994140625s\n",
      "Episode:24 Score:1.0 Time:0.33468008041381836s\n",
      "Episode:25 Score:1.0 Time:0.3933417797088623s\n",
      "Episode:26 Score:1.0 Time:0.4492068290710449s\n",
      "Episode:27 Score:-5.0 Time:0.37772297859191895s\n",
      "Episode:28 Score:-1.0 Time:0.35584187507629395s\n",
      "Episode:29 Score:1.0 Time:0.34467625617980957s\n",
      "Episode:30 Score:-1.0 Time:0.43150901794433594s\n",
      "Episode:31 Score:-1.0 Time:0.43495917320251465s\n",
      "Episode:32 Score:-7.0 Time:0.4303708076477051s\n",
      "Episode:33 Score:-1.0 Time:0.3728330135345459s\n",
      "Episode:34 Score:-1.0 Time:0.4399409294128418s\n",
      "Episode:35 Score:1.0 Time:0.42948007583618164s\n",
      "Episode:36 Score:1.0 Time:0.47811412811279297s\n",
      "Episode:37 Score:1.0 Time:0.42693400382995605s\n",
      "Episode:38 Score:1.0 Time:0.43351292610168457s\n",
      "Episode:39 Score:1.0 Time:0.46433568000793457s\n",
      "Episode:40 Score:1.0 Time:0.33107876777648926s\n",
      "Episode:41 Score:-1.0 Time:0.44893813133239746s\n",
      "Episode:42 Score:1.0 Time:0.40349602699279785s\n",
      "Episode:43 Score:-1.0 Time:0.44884300231933594s\n",
      "Episode:44 Score:1.0 Time:0.4345588684082031s\n",
      "Episode:45 Score:1.0 Time:0.3040127754211426s\n",
      "Episode:46 Score:1.0 Time:0.3596673011779785s\n",
      "Episode:47 Score:-1.0 Time:0.3974308967590332s\n",
      "Episode:48 Score:-1.0 Time:0.4496490955352783s\n",
      "Episode:49 Score:-10.0 Time:0.3910987377166748s\n",
      "Episode:50 Score:1.0 Time:0.42091894149780273s\n",
      "Average reward per episode -1.02\n"
     ]
    }
   ],
   "source": [
    "# Roda alguns episódios com política \"jogue uma carta aleatoriamente\"\n",
    "\n",
    "import time\n",
    "\n",
    "episodes = 50\n",
    "total_reward = 0\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    terminated = False\n",
    "    score = 0\n",
    "    start = time.time()\n",
    "\n",
    "    while not terminated:\n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, terminated, info = env.step(action)\n",
    "        score+=reward\n",
    "    total_reward += score\n",
    "    print('Episode:{} Score:{} Time:{}s'.format(episode, score, time.time() - start))\n",
    "print(\"Average reward per episode {}\".format(total_reward/episodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca969840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 24)                432       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 15)                375       \n",
      "=================================================================\n",
      "Total params: 1,407\n",
      "Trainable params: 1,407\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "\n",
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1, states[0])))\n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model\n",
    "\n",
    "model = build_model(states, actions)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "143f39f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 19:28:54.783377: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-24 19:28:54.798993: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc8d5d829c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-24 19:28:54.799009: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "WARNING:tensorflow:From /Users/chesterbr/.pyenv/versions/3.7.17/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "10000/10000 [==============================] - 143s 14ms/step - reward: -0.0205\n",
      "211 episodes - episode_reward: -0.972 [-14.000, 1.000] - loss: 58.016 - mae: 63.641 - mean_q: 78.649\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 172s 17ms/step - reward: -0.0083\n",
      "212 episodes - episode_reward: -0.392 [-11.000, 1.000] - loss: 3544.656 - mae: 498.051 - mean_q: 619.572\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 200s 20ms/step - reward: -0.0171\n",
      "209 episodes - episode_reward: -0.794 [-14.000, 1.000] - loss: 3188717.250 - mae: 10568.498 - mean_q: 13438.499\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 220s 22ms/step - reward: -0.0185\n",
      "207 episodes - episode_reward: -0.918 [-15.000, 1.000] - loss: 292090052608.000 - mae: 1551630.500 - mean_q: 2087385.125\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 230s 23ms/step - reward: -0.0198\n",
      "done, took 964.568 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1380ace90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MaskingDQNAgent(DQNAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MaskingDQNAgent, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def compute_q_values(self, state):\n",
    "        q_values = super().compute_q_values(state)\n",
    "        mask = np.array([1 if self._is_action_valid(state, action) else -np.inf for action in range(self.nb_actions)])\n",
    "        masked_q_values = q_values + mask\n",
    "        return masked_q_values\n",
    "\n",
    "    def _is_action_valid(self, state, action):\n",
    "        return state[0][0] == action or state[0][1] == action or state[0][2] == action\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = MaskingDQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  nb_actions=actions, nb_steps_warmup=1000, target_model_update=1e-2)\n",
    "    return dqn\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "# Original era 50K steps\n",
    "dqn.fit(env, nb_steps=50_000, visualize=False, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2663a67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 50 episodes ...\n",
      "Episode 1: reward: 1.000, steps: 48\n",
      "Episode 2: reward: -6.000, steps: 53\n",
      "Episode 3: reward: 1.000, steps: 50\n",
      "Episode 4: reward: -1.000, steps: 45\n",
      "Episode 5: reward: 1.000, steps: 43\n",
      "Episode 6: reward: 1.000, steps: 47\n",
      "Episode 7: reward: -1.000, steps: 43\n",
      "Episode 8: reward: -1.000, steps: 48\n",
      "Episode 9: reward: -1.000, steps: 51\n",
      "Episode 10: reward: -9.000, steps: 47\n",
      "Episode 11: reward: 1.000, steps: 56\n",
      "Episode 12: reward: 1.000, steps: 51\n",
      "Episode 13: reward: 1.000, steps: 50\n",
      "Episode 14: reward: 1.000, steps: 52\n",
      "Episode 15: reward: 1.000, steps: 44\n",
      "Episode 16: reward: -8.000, steps: 53\n",
      "Episode 17: reward: -1.000, steps: 44\n",
      "Episode 18: reward: -1.000, steps: 48\n",
      "Episode 19: reward: 1.000, steps: 48\n",
      "Episode 20: reward: -1.000, steps: 53\n",
      "Episode 21: reward: -1.000, steps: 50\n",
      "Episode 22: reward: -6.000, steps: 47\n",
      "Episode 23: reward: -1.000, steps: 50\n",
      "Episode 24: reward: -1.000, steps: 47\n",
      "Episode 25: reward: 1.000, steps: 50\n",
      "Episode 26: reward: -1.000, steps: 36\n",
      "Episode 27: reward: 1.000, steps: 49\n",
      "Episode 28: reward: 1.000, steps: 49\n",
      "Episode 29: reward: -1.000, steps: 55\n",
      "Episode 30: reward: -1.000, steps: 40\n",
      "Episode 31: reward: 1.000, steps: 52\n",
      "Episode 32: reward: -11.000, steps: 51\n",
      "Episode 33: reward: -1.000, steps: 45\n",
      "Episode 34: reward: 1.000, steps: 50\n",
      "Episode 35: reward: -1.000, steps: 54\n",
      "Episode 36: reward: -1.000, steps: 49\n",
      "Episode 37: reward: 1.000, steps: 46\n",
      "Episode 38: reward: -16.000, steps: 53\n",
      "Episode 39: reward: -1.000, steps: 43\n",
      "Episode 40: reward: 1.000, steps: 56\n",
      "Episode 41: reward: -1.000, steps: 48\n",
      "Episode 42: reward: 1.000, steps: 47\n",
      "Episode 43: reward: -1.000, steps: 41\n",
      "Episode 44: reward: 1.000, steps: 58\n",
      "Episode 45: reward: -1.000, steps: 47\n",
      "Episode 46: reward: -1.000, steps: 47\n",
      "Episode 47: reward: -1.000, steps: 31\n",
      "Episode 48: reward: -12.000, steps: 54\n",
      "Episode 49: reward: 1.000, steps: 52\n",
      "Episode 50: reward: -1.000, steps: 44\n",
      "Average reward per episode: -1.42\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=50, visualize=False)\n",
    "print(\"Average reward per episode: {}\".format(np.mean(scores.history['episode_reward'])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
