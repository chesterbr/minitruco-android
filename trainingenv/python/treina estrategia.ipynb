{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bc71e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "from py4j.java_gateway import JavaGateway\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "534058d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conecta no servidor de treinamento\n",
    "\n",
    "gateway = JavaGateway()\n",
    "minitruco_java = gateway.entry_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c88c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chesterbr/.pyenv/versions/3.7.17/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# Env customizado do gym que encapsupla o servidor\n",
    "\n",
    "class MinitrucoEnv(Env):\n",
    "    def __init__(self):\n",
    "        # Ações que podemos tomar: jogar a carta 0, a carta 1 ou a carta 2\n",
    "        self.action_space = Discrete(3)\n",
    "\n",
    "        # Espaço de observação: os 3 primeiros elementos são as cartas em mãos,\n",
    "        # o restante são os valores das cartas na mesa, pontuações, etc.\n",
    "        # Detalhes em SituacaoJogo.java#ranges e #toObservacao\n",
    "        low, high = np.array(minitruco_java.ranges()).transpose()\n",
    "        self.observation_space = Box(low=np.array(low), high=np.array(high), dtype=np.float32)\n",
    "\n",
    "        self.episodio = None\n",
    "        self.state = None\n",
    "\n",
    "    def _get_obs(self):\n",
    "        values = self.episodio.estado().split()\n",
    "        return [float(val) for val in values]\n",
    "        # return np.array(float_values[24:27], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        action = int(action)\n",
    "        last_observation = self.state\n",
    "        cartaJogada = last_observation[action]\n",
    "        if cartaJogada == -1:\n",
    "            # Jogada inválida (não deve acontecer no Keras por causa do masking, mas\n",
    "            # se rolar por conta do teste inicial, só ignora e mantém no mesmo estado)\n",
    "            return last_observation, 0, False, {}\n",
    "\n",
    "        self.episodio.executa(action)\n",
    "        estado_str = self.episodio.estado()\n",
    "        if estado_str == \"EQUIPE 1 VENCEU\":\n",
    "            return last_observation, 1.0, True, {}\n",
    "        if estado_str == \"EQUIPE 2 VENCEU\":\n",
    "            return last_observation, -1.0, True, {}\n",
    "\n",
    "        self.state = self._get_obs()\n",
    "\n",
    "        reward = 0.0\n",
    "        terminated = False\n",
    "\n",
    "        return self.state, reward, terminated, {}\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if self.episodio is not None:\n",
    "            self.episodio.finaliza()\n",
    "\n",
    "#         super().reset(seed=seed) # required by check_env\n",
    "\n",
    "        self.episodio = minitruco_java.novoEpisodio()\n",
    "        self.state = self._get_obs()\n",
    "\n",
    "        return self.state\n",
    "\n",
    "\n",
    "env = MinitrucoEnv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19da8770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-1.0 Time:0.48358702659606934s\n",
      "Episode:2 Score:-1.0 Time:0.47749781608581543s\n",
      "Episode:3 Score:1.0 Time:0.4682190418243408s\n",
      "Episode:4 Score:1.0 Time:0.516963005065918s\n",
      "Episode:5 Score:-1.0 Time:0.5109500885009766s\n",
      "Episode:6 Score:-1.0 Time:0.5222787857055664s\n",
      "Episode:7 Score:1.0 Time:0.4657111167907715s\n",
      "Episode:8 Score:1.0 Time:0.5382301807403564s\n",
      "Episode:9 Score:1.0 Time:0.4107491970062256s\n",
      "Episode:10 Score:-1.0 Time:0.3027799129486084s\n",
      "Episode:11 Score:-1.0 Time:0.40374207496643066s\n",
      "Episode:12 Score:1.0 Time:0.473527193069458s\n",
      "Episode:13 Score:1.0 Time:0.4430389404296875s\n",
      "Episode:14 Score:-1.0 Time:0.45159912109375s\n",
      "Episode:15 Score:1.0 Time:0.503068208694458s\n",
      "Episode:16 Score:-1.0 Time:0.39084696769714355s\n",
      "Episode:17 Score:-1.0 Time:0.4611020088195801s\n",
      "Episode:18 Score:1.0 Time:0.5063929557800293s\n",
      "Episode:19 Score:-1.0 Time:0.5153799057006836s\n",
      "Episode:20 Score:-1.0 Time:0.4199531078338623s\n",
      "Episode:21 Score:-1.0 Time:0.4751548767089844s\n",
      "Episode:22 Score:-1.0 Time:0.4129340648651123s\n",
      "Episode:23 Score:-1.0 Time:0.41384291648864746s\n",
      "Episode:24 Score:-1.0 Time:0.32288193702697754s\n",
      "Episode:25 Score:1.0 Time:0.4449269771575928s\n",
      "Episode:26 Score:-1.0 Time:0.515143871307373s\n",
      "Episode:27 Score:-1.0 Time:0.42470335960388184s\n",
      "Episode:28 Score:1.0 Time:0.41005921363830566s\n",
      "Episode:29 Score:-1.0 Time:0.36554789543151855s\n",
      "Episode:30 Score:-1.0 Time:0.4743080139160156s\n",
      "Episode:31 Score:1.0 Time:0.4169778823852539s\n",
      "Episode:32 Score:-1.0 Time:0.33321595191955566s\n",
      "Episode:33 Score:-1.0 Time:0.41562390327453613s\n",
      "Episode:34 Score:-1.0 Time:0.4265739917755127s\n",
      "Episode:35 Score:-1.0 Time:0.4790799617767334s\n",
      "Episode:36 Score:-1.0 Time:0.3956010341644287s\n",
      "Episode:37 Score:-1.0 Time:0.3986659049987793s\n",
      "Episode:38 Score:-1.0 Time:0.4383392333984375s\n",
      "Episode:39 Score:-1.0 Time:0.2752699851989746s\n",
      "Episode:40 Score:-1.0 Time:0.42903685569763184s\n",
      "Episode:41 Score:-1.0 Time:0.4687318801879883s\n",
      "Episode:42 Score:-1.0 Time:0.4724421501159668s\n",
      "Episode:43 Score:1.0 Time:0.4033360481262207s\n",
      "Episode:44 Score:-1.0 Time:0.43337082862854004s\n",
      "Episode:45 Score:-1.0 Time:0.39715099334716797s\n",
      "Episode:46 Score:-1.0 Time:0.43268895149230957s\n",
      "Episode:47 Score:-1.0 Time:0.4438958168029785s\n",
      "Episode:48 Score:1.0 Time:0.4460868835449219s\n",
      "Episode:49 Score:-1.0 Time:0.37401294708251953s\n",
      "Episode:50 Score:-1.0 Time:0.4748239517211914s\n",
      "Average reward per episode -0.44\n"
     ]
    }
   ],
   "source": [
    "# Roda alguns episódios com política \"jogue uma carta aleatoriamente\"\n",
    "\n",
    "import time\n",
    "\n",
    "episodes = 50\n",
    "total_reward = 0\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    terminated = False\n",
    "    score = 0\n",
    "    start = time.time()\n",
    "\n",
    "    while not terminated:\n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, terminated, info = env.step(action)\n",
    "        score+=reward\n",
    "    total_reward += score\n",
    "    print('Episode:{} Score:{} Time:{}s'.format(episode, score, time.time() - start))\n",
    "print(\"Average reward per episode {}\".format(total_reward/episodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca969840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                672       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 1,347\n",
      "Trainable params: 1,347\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "\n",
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1, states[0])))\n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model\n",
    "\n",
    "model = build_model(states, actions)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "143f39f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 18:28:49.504254: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-26 18:28:49.525630: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f7f7bb0b680 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-26 18:28:49.525651: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "WARNING:tensorflow:From /Users/chesterbr/.pyenv/versions/3.7.17/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "    6/10000 [..............................] - ETA: 2:56 - reward: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chesterbr/.pyenv/versions/3.7.17/lib/python3.7/site-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 184s 18ms/step - reward: -0.0081\n",
      "197 episodes - episode_reward: -0.411 [-1.000, 1.000] - loss: 0.046 - mae: 1.259 - mean_q: 2.201\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 197s 20ms/step - reward: -0.0046\n",
      "198 episodes - episode_reward: -0.232 [-1.000, 1.000] - loss: 0.014 - mae: 0.827 - mean_q: 1.346\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0070\n",
      "198 episodes - episode_reward: -0.354 [-1.000, 1.000] - loss: 0.010 - mae: 0.585 - mean_q: 0.919\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 201s 20ms/step - reward: -0.0073\n",
      "199 episodes - episode_reward: -0.367 [-1.000, 1.000] - loss: 0.010 - mae: 0.551 - mean_q: 0.822\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      " 1417/10000 [===>..........................] - ETA: 3:03 - reward: -0.0014"
     ]
    }
   ],
   "source": [
    "class MaskingDQNAgent(DQNAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MaskingDQNAgent, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def compute_q_values(self, state):\n",
    "        q_values = super().compute_q_values(state)\n",
    "        mask = np.array([1 if self._is_action_valid(state, action) else -np.inf for action in range(self.nb_actions)])\n",
    "        masked_q_values = q_values + mask\n",
    "        return masked_q_values\n",
    "\n",
    "    def _is_action_valid(self, state, action):\n",
    "        return state[0][action] != -1\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = MaskingDQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "# Original era 50K steps, vamos devagar\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2663a67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 50 episodes ...\n",
      "Episode 1: reward: 1.000, steps: 55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2: reward: 1.000, steps: 52\n",
      "Episode 3: reward: -1.000, steps: 43\n",
      "Episode 4: reward: -1.000, steps: 60\n",
      "Episode 5: reward: -1.000, steps: 49\n",
      "Episode 6: reward: -1.000, steps: 55\n",
      "Episode 7: reward: 1.000, steps: 54\n",
      "Episode 8: reward: 1.000, steps: 51\n",
      "Episode 9: reward: -1.000, steps: 49\n",
      "Episode 10: reward: 1.000, steps: 44\n",
      "Episode 11: reward: -1.000, steps: 49\n",
      "Episode 12: reward: -1.000, steps: 42\n",
      "Episode 13: reward: -1.000, steps: 50\n",
      "Episode 14: reward: -1.000, steps: 51\n",
      "Episode 15: reward: 1.000, steps: 49\n",
      "Episode 16: reward: -1.000, steps: 40\n",
      "Episode 17: reward: -1.000, steps: 60\n",
      "Episode 18: reward: -1.000, steps: 42\n",
      "Episode 19: reward: -1.000, steps: 59\n",
      "Episode 20: reward: -1.000, steps: 50\n",
      "Episode 21: reward: -1.000, steps: 39\n",
      "Episode 22: reward: 1.000, steps: 54\n",
      "Episode 23: reward: -1.000, steps: 53\n",
      "Episode 24: reward: -1.000, steps: 44\n",
      "Episode 25: reward: -1.000, steps: 50\n",
      "Episode 26: reward: 1.000, steps: 53\n",
      "Episode 27: reward: 1.000, steps: 55\n",
      "Episode 28: reward: -1.000, steps: 41\n",
      "Episode 29: reward: 1.000, steps: 61\n",
      "Episode 30: reward: -1.000, steps: 38\n",
      "Episode 31: reward: 1.000, steps: 53\n",
      "Episode 32: reward: -1.000, steps: 47\n",
      "Episode 33: reward: -1.000, steps: 41\n",
      "Episode 34: reward: 1.000, steps: 55\n",
      "Episode 35: reward: -1.000, steps: 53\n",
      "Episode 36: reward: 1.000, steps: 55\n",
      "Episode 37: reward: 1.000, steps: 44\n",
      "Episode 38: reward: -1.000, steps: 52\n",
      "Episode 39: reward: 1.000, steps: 40\n",
      "Episode 40: reward: -1.000, steps: 44\n",
      "Episode 41: reward: 1.000, steps: 51\n",
      "Episode 42: reward: 1.000, steps: 56\n",
      "Episode 43: reward: -1.000, steps: 54\n",
      "Episode 44: reward: 1.000, steps: 51\n",
      "Episode 45: reward: 1.000, steps: 50\n",
      "Episode 46: reward: -1.000, steps: 59\n",
      "Episode 47: reward: -1.000, steps: 52\n",
      "Episode 48: reward: -1.000, steps: 44\n",
      "Episode 49: reward: -1.000, steps: 59\n",
      "Episode 50: reward: 1.000, steps: 36\n",
      "Average reward per episode: -0.2\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=50, visualize=False)\n",
    "print(\"Average reward per episode: {}\".format(np.mean(scores.history['episode_reward'])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
