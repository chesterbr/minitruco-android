{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc71e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "from py4j.java_gateway import JavaGateway\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534058d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conecta no servidor de treinamento\n",
    "\n",
    "gateway = JavaGateway()\n",
    "minitruco_java = gateway.entry_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c88c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env customizado do gym que encapsupla o servidor\n",
    "\n",
    "class MinitrucoEnv(Env):\n",
    "    def __init__(self):\n",
    "        # Ações que podemos tomar: jogar a carta 0, a carta 1 ou a carta 2\n",
    "        self.action_space = Discrete(3)\n",
    "\n",
    "        # Espaço de observação: os 3 primeiros elementos são as cartas em mãos,\n",
    "        # o restante são os valores das cartas na mesa, pontuações, etc.\n",
    "        # Detalhes em SituacaoJogo.java#ranges e #toObservacao\n",
    "        low, high = np.array(minitruco_java.ranges()).transpose()\n",
    "        self.observation_space = Box(low=np.array(low), high=np.array(high), dtype=np.float32)\n",
    "\n",
    "        self.episodio = None\n",
    "        self.state = None\n",
    "\n",
    "    def _get_obs(self):\n",
    "        values = self.episodio.estado().split()\n",
    "        return [float(val) for val in values]\n",
    "        # return np.array(float_values[24:27], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        action = int(action)\n",
    "        last_observation = self.state\n",
    "        cartaJogada = last_observation[action]\n",
    "        if cartaJogada == -1:\n",
    "            # Jogada inválida (não deve acontecer no Keras por causa do masking, mas\n",
    "            # se rolar por conta do teste inicial, só ignora e mantém no mesmo estado)\n",
    "            return last_observation, 0, False, {}\n",
    "\n",
    "        self.episodio.executa(action)\n",
    "        estado_str = self.episodio.estado()\n",
    "        if estado_str == \"EQUIPE 1 VENCEU\":\n",
    "            return last_observation, 1.0, True, {}\n",
    "        if estado_str == \"EQUIPE 2 VENCEU\":\n",
    "            return last_observation, -1.0, True, {}\n",
    "\n",
    "        self.state = self._get_obs()\n",
    "        ganhoPontosEquipe1 = self.state[13] - last_observation[13]\n",
    "        ganhoPontosEquipe2 = self.state[14] - last_observation[14]\n",
    "\n",
    "        # Dá 1 ponto de recompensa se a equipe 1 ganhou pontos e -1 se a equipe 2 ganhou pontos\n",
    "        reward = ganhoPontosEquipe1 - ganhoPontosEquipe2\n",
    "        terminated = False\n",
    "\n",
    "        return self.state, reward, terminated, {}\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if self.episodio is not None:\n",
    "            self.episodio.finaliza()\n",
    "\n",
    "#         super().reset(seed=seed) # required by check_env\n",
    "\n",
    "        self.episodio = minitruco_java.novoEpisodio()\n",
    "        self.state = self._get_obs()\n",
    "\n",
    "        return self.state\n",
    "\n",
    "\n",
    "env = MinitrucoEnv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19da8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roda alguns episódios com política \"jogue uma carta aleatoriamente\"\n",
    "\n",
    "import time\n",
    "\n",
    "episodes = 50\n",
    "total_reward = 0\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    terminated = False\n",
    "    score = 0\n",
    "    start = time.time()\n",
    "\n",
    "    while not terminated:\n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, terminated, info = env.step(action)\n",
    "        score+=reward\n",
    "    total_reward += score\n",
    "    print('Episode:{} Score:{} Time:{}s'.format(episode, score, time.time() - start))\n",
    "print(\"Average reward per episode {}\".format(total_reward/episodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca969840",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "\n",
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1, states[0])))\n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model\n",
    "\n",
    "model = build_model(states, actions)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143f39f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskingDQNAgent(DQNAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MaskingDQNAgent, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def compute_q_values(self, state):\n",
    "        q_values = super().compute_q_values(state)\n",
    "        mask = np.array([1 if self._is_action_valid(state, action) else -np.inf for action in range(self.nb_actions)])\n",
    "        masked_q_values = q_values + mask\n",
    "        return masked_q_values\n",
    "\n",
    "    def _is_action_valid(self, state, action):\n",
    "        return state[0][action] != -1\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = MaskingDQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  nb_actions=actions, nb_steps_warmup=1000, target_model_update=1e-2)\n",
    "    return dqn\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "# Original era 50K steps, vamos devagar\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2663a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dqn.test(env, nb_episodes=50, visualize=False)\n",
    "print(\"Average reward per episode: {}\".format(np.mean(scores.history['episode_reward'])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
